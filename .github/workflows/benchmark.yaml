name: Benchmark Suite

on:
  pull_request:
    branches: [main, master]
    paths:
      - 'faster_eth_utils/**/*.py'
      - 'benchmarks/**/*.py'
      - '.github/workflows/benchmark.yaml'
      - 'pyproject.toml'
      - 'setup.py'
  push:
    branches: [main, master]
    paths:
      - 'faster_eth_utils/**/*.py'
      - 'benchmarks/**/*.py'
      - '.github/workflows/benchmark.yaml'
      - 'pyproject.toml'
      - 'setup.py'
  workflow_dispatch:
    inputs:
      run-type:
        description: "Select which benchmarks to run"
        required: false
        default: "both"
        type: choice
        options:
          - both
          - codspeed
          - comparison

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-wheel:
    name: Build wheel for Python3.14
    runs-on: ubuntu-latest
    outputs:
      artifact-name: ${{ steps.mypycify.outputs.artifact-name }}
    steps:
      - uses: actions/checkout@v5
      - name: Build wheel with mypycify
        id: mypycify
        uses: BobTheBuidler/mypycify@master
        with:
          python-version: "3.14"
          hash-key: |
            pyproject.toml
            setup.py
            tox.ini
            faster_eth_utils/**/*.py
          pip-cache-dependency-path: |
            pyproject.toml
            setup.py
            tox.ini
          ccache: true

  codspeed-benchmark:
    needs: build-wheel
    name: Run CodSpeed Benchmarks
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.run-type == 'both' || github.event.inputs.run-type == '' || github.event.inputs.run-type == 'codspeed' }}
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.14"
          cache: pip
          cache-dependency-path: |
            pyproject.toml
            setup.py
            tox.ini

      - name: Download wheel
        uses: actions/download-artifact@v6
        with:
          name: ${{ needs.build-wheel.outputs.artifact-name }}
          path: .

      - name: Install built wheel and codspeed dependencies
        run: |
          python -m pip install --upgrade pip
          pip install "$(find . -name '*.whl')" -r requirements-codspeed.txt

      - name: Remove Python files from test env
        run: rm -r faster_eth_utils

      - name: Run CodSpeed
        uses: CodSpeedHQ/action@v4
        with:
          mode: instrumentation
          run: pytest --codspeed -k "test_faster_" --test-group=${{ matrix.shard }} --test-group-count=10 benchmarks/

  comparison-benchmark:
    name: Run Comparison Benchmarks
    runs-on: ubuntu-latest
    needs: build-wheel
    if: ${{ github.event.inputs.run-type == 'both' || github.event.inputs.run-type == '' || github.event.inputs.run-type == 'comparison' }}
    steps:
      - uses: actions/checkout@v5
      
      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.14"
          cache: pip
          cache-dependency-path: |
            pyproject.toml
            setup.py
            tox.ini

      - name: Download built wheel artifact
        uses: actions/download-artifact@v6
        with:
          name: ${{ needs.build-wheel.outputs.artifact-name }}
          path: .

      - name: Install built wheel and benchmark dependencies
        run: pip install ./*.whl -r requirements.txt
          
      - name: Remove Python files from test env
        run: |
          rm -r faster_eth_utils
          
      - name: Run Pytest Benchmark & Save Output
        run: pytest --benchmark-only --benchmark-json=benchmark.json benchmarks/
        
      - name: Upload Pytest Benchmark Results
        uses: actions/upload-artifact@v5
        with:
          name: pytest-benchmark-results
          path: benchmark.json
          
      - name: Parse Pytest Benchmark Output
        run: python scripts/benchmark/parse_benchmark_output.py benchmark.json pytest_benchmark_results.json
        
      - name: Compare Pytest Benchmark Results
        run: python scripts/benchmark/compare_benchmark_results.py pytest_benchmark_results.json pytest_benchmark_diff.json
        
      - name: Generate Markdown Benchmark Results
        run: python scripts/benchmark/generate_benchmark_markdown.py
        
      - name: Commit and Push Markdown Benchmark Results
        continue-on-error: true
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git fetch origin ${{ github.head_ref || github.ref_name }}
          git checkout ${{ github.head_ref || github.ref_name }}
          git add benchmarks/results/*.md
          if git diff --cached --quiet; then
            echo "No changes to commit."
          else
            git commit -m "Update benchmark results [skip ci]"
            git push origin HEAD:${{ github.head_ref || github.ref_name }}
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Upload Markdown Benchmark Results
        uses: actions/upload-artifact@v5
        with:
          name: markdown-benchmark-results
          path: benchmarks/results/*.md
          
      - name: Upload Pytest Benchmark Diff
        uses: actions/upload-artifact@v5
        with:
          name: pytest-benchmark-diff
          path: pytest_benchmark_diff.json
          
      - name: Post Pytest Benchmark Diff as PR Comment
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const diff = JSON.parse(fs.readFileSync('pytest_benchmark_diff.json', 'utf8'));
            let body = '### Pytest Benchmark Diff\n\n';
            // Get repo and branch info from context
            const repo = `${context.repo.owner}/${context.repo.repo}`;
            const branch = context.payload.pull_request ? context.payload.pull_request.head.ref : context.ref.replace('refs/heads/', '');
            for (const [submodule, groupDiffs] of Object.entries(diff)) {
              // Convert submodule name to submodule file path (e.g., faster_eth_utils.abi -> faster_eth_utils/types.py)
              let submoduleFile = "unknown";
              let benchmarkFile = "unknown";
              const m = submodule.match(/^faster_eth_utils\.(.+)$/);
              if (m) {
                submoduleFile = `faster_eth_utils/${m[1]}.py`;
                benchmarkFile = `benchmarks/test_${m[1]}_benchmarks.py`;
              }
              const submoduleUrl = `https://github.com/${repo}/blob/${branch}/${submoduleFile}`;
              const benchmarkUrl = `https://github.com/${repo}/blob/${branch}/${benchmarkFile}`;
              body += `#### [${submodule}](${submoduleUrl}) - [view benchmarks](${benchmarkUrl})\n`;
              body += '| Function | Reference Mean | Faster Mean | % Change | Speedup (%) | x Faster | Faster |\n';
              body += '|----------|---------------|-------------|----------|-------------|----------|--------|\n';
              for (const [group, data] of Object.entries(groupDiffs).sort(([a], [b]) => a.localeCompare(b))) {
                if (data.percent_change !== null && data.percent_change !== undefined) {
                  let emoji = '➖';
                  if (data.percent_change > 0) emoji = '✅';
                  else if (data.percent_change < 0) emoji = '❌';
                  const percentChange = data.percent_change !== undefined && data.percent_change !== null
                    ? `${data.percent_change.toFixed(2)}%` : '';
                  const speedupPercent = data.speedup_percent !== undefined && data.speedup_percent !== null
                    ? `${data.speedup_percent.toFixed(2)}%` : '';
                  const speedupX = data.speedup_x !== undefined && data.speedup_x !== null && isFinite(data.speedup_x)
                    ? `${data.speedup_x.toFixed(2)}x` : '';
                  body += `| \`${group}\` | ${data.reference_mean} | ${data.faster_mean} | ${percentChange} | ${speedupPercent} | ${speedupX} | ${emoji} |\n`;
                } else if (data.note) {
                  body += `| \`${group}\` |  |  |  |  |  | ➖ |\n`;
                }
              }
              body += '\n';
            }
            github.rest.issues.createComment({
              ...context.repo,
              issue_number: context.issue.number,
              body
            });
        if: github.event_name == 'pull_request'
